{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/taegyu/git_repo/ipirec\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from IPython.display import clear_output\n",
    "\n",
    "__FILE_DIR_PATH = os.path.dirname(__vsc_ipynb_file__) \\\n",
    "    if __IPYTHON__ \\\n",
    "    else os.path.dirname(__file__)\n",
    "    \n",
    "WORKSPACE_HOME = __FILE_DIR_PATH.replace(\n",
    "    f\"/{os.path.basename(__FILE_DIR_PATH)}\", \"\")\n",
    "WORKSPACE_HOME = WORKSPACE_HOME.replace(\"/trunk\", \"\")\n",
    "DATASET_DIR_HOME = f\"{WORKSPACE_HOME}/data/colley\"\n",
    "\n",
    "print(WORKSPACE_HOME)\n",
    "sys.path.append(WORKSPACE_HOME)\n",
    "\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from tqdm import tqdm\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from core import *\n",
    "from ipirec import *\n",
    "from colley import *\n",
    "\n",
    "# plt.rcParams[\"font.family\"] = \"AppleGothic\"\n",
    "plt.rcParams[\"font.family\"] = \"NanumGothic\"\n",
    "mpl.rcParams[\"axes.unicode_minus\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPT & ALLOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IO] /home/taegyu/git_repo/ipirec/data/colley/train_2_view_list.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LOAD] train_2_view_list.csv: 100%|██████████| 46568/46568 [00:00<00:00, 84445.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IO] /home/taegyu/git_repo/ipirec/data/colley/train_2_like_list.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LOAD] train_2_like_list.csv: 100%|██████████| 22362/22362 [00:00<00:00, 84211.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IO] /home/taegyu/git_repo/ipirec/data/colley/train_2_purchase_list.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LOAD] train_2_purchase_list.csv: 100%|██████████| 12349/12349 [00:00<00:00, 83231.50it/s]\n"
     ]
    }
   ],
   "source": [
    "_FOLD_SET_ID = 2\n",
    "top_n_conditions = [n for n in range(3, 37, 2)]\n",
    "\n",
    "_TEST_SET_FILES_LIST = [\n",
    "    str.format(\n",
    "        \"{0}/test_{1}_{2}_list.csv\",\n",
    "        DATASET_DIR_HOME,\n",
    "        _FOLD_SET_ID,\n",
    "        DecisionType.to_str(d),\n",
    "    )\n",
    "    for d in [\n",
    "        DecisionType.E_LIKE,\n",
    "        DecisionType.E_PURCHASE,\n",
    "    ]\n",
    "]\n",
    "\n",
    "dataset = ColleyFilteredDataSet(dataset_dir_path=DATASET_DIR_HOME)\n",
    "dataset._load_metadata_()\n",
    "for decision_type in DecisionType:\n",
    "    dataset.append_decisions(\n",
    "        file_path=str.format(\n",
    "            \"{0}/train_{1}_{2}_list.csv\",\n",
    "            DATASET_DIR_HOME,\n",
    "            _FOLD_SET_ID,\n",
    "            DecisionType.to_str(decision_type),\n",
    "        ),\n",
    "        decision_type=decision_type,\n",
    "    )\n",
    "dataset.__id_index_mapping__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/taegyu/git_repo/ipirec/data/colley/user_interest_tag_list.csv\n"
     ]
    }
   ],
   "source": [
    "dataset.append_interest_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user: UserEntity = dataset.user_dict[692466]\n",
    "user.top_n_decision_tags_set\n",
    "# user.top_n_decision_tags_set.difference_update(user.set_of_interest_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CorrelationModel.preprocess()\n",
      "CorrelationModel.top_n_decision_tags()\n",
      "CorrelationModel.mean_freq_tags()\n",
      "CorrelationModel.item_based_tags_corr()\n",
      "CorrelationModel.user_based_tags_corr()\n",
      "CorrelationModel.tags_score()\n"
     ]
    }
   ],
   "source": [
    "top_n_tags = 5\n",
    "\n",
    "model_params = IPIRecModel.create_models_parameters(\n",
    "    top_n_tags=top_n_tags,\n",
    "    co_occur_items_threshold=4,\n",
    ")\n",
    "model = IPIRecModel(\n",
    "    dataset=dataset,\n",
    "    model_params=model_params,\n",
    ")\n",
    "model.analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdjustedBiasedCorrelationEstimator.append_biases()\n"
     ]
    }
   ],
   "source": [
    "frob_norm = 1.0\n",
    "score_iters = 10\n",
    "weight_iters = 5\n",
    "\n",
    "estimator_params = AdjustedBiasedCorrelationEstimator.create_models_parameters(\n",
    "    score_iterations=score_iters,\n",
    "    score_learning_rate=10 ** -2,\n",
    "    score_generalization=10 ** -4,\n",
    "    weight_iterations=weight_iters,\n",
    "    weight_learning_rate=10 ** -3,\n",
    "    weight_generalization=1.0,\n",
    "    frob_norm=frob_norm,\n",
    "    default_voting=0.0,\n",
    ")\n",
    "estimator = AdjustedBiasedCorrelationEstimator(\n",
    "    model=model,\n",
    "    model_params=estimator_params,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for decision_type in DecisionType:\n",
    "    for _ in tqdm(\n",
    "        iterable=range(score_iters),\n",
    "        desc=f\"{DecisionType.to_str(decision_type)}\",\n",
    "        total=score_iters,\n",
    "    ):\n",
    "        _L = estimator._adjust_tags_corr_(\n",
    "            decision_type=decision_type,\n",
    "        )\n",
    "        print(f\"{_L}\")\n",
    "        # estimator._personalization_(\n",
    "        #     target_decision=decision_type,\n",
    "        # )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_uidx = estimator.user_id_to_idx.get(692466, -1)\n",
    "if _uidx != -1:\n",
    "    print(estimator.arr_users_tags_map[_uidx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user: UserEntity = dataset.user_dict[692466]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_iidx = estimator.item_id_to_idx.get(472, -1)\n",
    "if _iidx != -1:\n",
    "    print(estimator.arr_items_tags_map[_iidx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdjustedBiasedCorrelationEstimator.append_biases()\n",
      "[1 S] L: 0.6894625961813272\n",
      "S: 0.12315779947292778\n",
      "W: 0.0\n",
      "0.6894625961813272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[view] Adjust: 100%|██████████| 39992/39992 [00:06<00:00, 5919.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 W] L: 0.9987437682391035\n",
      "S: 0.09011785644077157\n",
      "W: 0.02918587438762188\n",
      "1.169582506198683\n",
      "AdjustedBiasedCorrelationEstimator.append_biases()\n",
      "[2 S] L: 0.7956773834984967\n",
      "S: 0.12342493682073691\n",
      "W: 0.02918587438762188\n",
      "0.9665161214580762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[view] Adjust: 100%|██████████| 39992/39992 [00:06<00:00, 5951.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 W] L: 0.9990000601528866\n",
      "S: 0.10302121897816942\n",
      "W: 0.02989768236875534\n",
      "1.1719095230771446\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_log_list = list()\n",
    "_ITER = 0\n",
    "while True:\n",
    "    _ITER += 1\n",
    "    estimator.__append_biases__()\n",
    "    _L = estimator._adjust_tags_corr_(DecisionType.E_VIEW)\n",
    "    _S = np.std(estimator.arr_tags_score)\n",
    "    _W = np.std(estimator.arr_user_idx_to_weights)\n",
    "    print(\n",
    "        str.format(\n",
    "            \"[{0} S] L: {1}\\nS: {2}\\nW: {3}\",\n",
    "                _ITER,\n",
    "                _L,\n",
    "                _S,\n",
    "                _W,\n",
    "                )\n",
    "        )\n",
    "    # __L = _L +(( _S + _W)**(2**-1))\n",
    "    __L = _L +(_W**(2**-1))\n",
    "    print(__L)\n",
    "    _L = estimator._personalization_(DecisionType.E_VIEW)\n",
    "    _S = np.std(estimator.arr_tags_score)\n",
    "    _W = np.std(estimator.arr_user_idx_to_weights)\n",
    "    # _S = np.sum(np.abs(estimator.arr_tags_score))\n",
    "    # _W = np.sum(np.abs(estimator.arr_user_idx_to_weights))\n",
    "    print(\n",
    "        str.format(\n",
    "            \"[{0} W] L: {1}\\nS: {2}\\nW: {3}\",\n",
    "                _ITER,\n",
    "                _L,\n",
    "                _S,\n",
    "                _W,\n",
    "                )\n",
    "        )\n",
    "    # __L = _L +(( _S + _W)**(2**-1))\n",
    "    __L = _L +(_W**(2**-1))\n",
    "    print(__L)\n",
    "    loss_log_list.append(_L)\n",
    "    _min = min(loss_log_list)\n",
    "    if _min < _L:\n",
    "        #_estimator: AdjustedBiasedCorrelationEstimator = copy.deepcopy(estimator)\n",
    "        break\n",
    "    \n",
    "inst = BaseAction(user_id=692466, item_id=472)\n",
    "inst = estimator._estimate_(inst)\n",
    "inst.estimated_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ITER = 100\n",
    "# 21881.16485981459\n",
    "# 22906.855353528248 \n",
    "# [P -> S] 22847.84812020912 >> 25977.561489833748 >> 25564.655994604418\n",
    "decision_type = DecisionType.E_VIEW\n",
    "__E = list()\n",
    "for _ in tqdm(\n",
    "        iterable=range(_ITER),\n",
    "        desc=f\"{DecisionType.to_str(decision_type)}\",\n",
    "        total=_ITER,\n",
    "    ):\n",
    "        _L = estimator._adjust_tags_corr_(\n",
    "            decision_type=decision_type,\n",
    "        )\n",
    "        print(f\"{_L}\")\n",
    "        __E.append(_L)\n",
    "        if min(__E) < _L:\n",
    "            __E.clear()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ITER = 100\n",
    "# 7841.645698883881 >> 7956.079589877575\n",
    "# 8582.921496806883\n",
    "decision_type = DecisionType.E_LIKE\n",
    "__E = list()\n",
    "for _ in tqdm(\n",
    "        iterable=range(_ITER),\n",
    "        desc=f\"{DecisionType.to_str(decision_type)}\",\n",
    "        total=_ITER,\n",
    "    ):\n",
    "        _L = estimator._adjust_tags_corr_(\n",
    "            decision_type=decision_type,\n",
    "        )\n",
    "        print(f\"{_L}\")\n",
    "        __E.append(_L)\n",
    "        if min(__E) < _L:\n",
    "            __E.clear()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ITER = 100\n",
    "## 8901.497879940547\n",
    "# 8959.261042943654 >> 9056.971447119242\n",
    "# 9147.675861340429\n",
    "# [P->S] 8965.367465171059 >> 9147.313206731318\n",
    "# 9209.555742799665\n",
    "__E = list()\n",
    "decision_type = DecisionType.E_PURCHASE\n",
    "for _ in tqdm(\n",
    "        iterable=range(_ITER),\n",
    "        desc=f\"{DecisionType.to_str(decision_type)}\",\n",
    "        total=_ITER,\n",
    "    ):\n",
    "        if (__IPYTHON__) and (_ % 5 == 0):\n",
    "            clear_output(wait=True)\n",
    "        _L = estimator._adjust_tags_corr_(\n",
    "            decision_type=decision_type,\n",
    "        )\n",
    "        print(f\"{_L}\")\n",
    "        __E.append(_L)\n",
    "        if min(__E) < _L:\n",
    "            __E.clear()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recommender.prediction(): 100%|██████████| 24270/24270 [04:09<00:00, 97.25it/s] \n"
     ]
    }
   ],
   "source": [
    "recommender = ScoreBasedRecommender(\n",
    "    estimator=estimator,\n",
    ")\n",
    "recommender.prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "recommender = ELABasedRecommender(\n",
    "    estimator=estimator,\n",
    ")\n",
    "recommender.prediction()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVAL - Recommended items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IO] /home/taegyu/git_repo/ipirec/data/colley/test_2_like_list.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LOAD] test_2_like_list.csv: 100%|██████████| 5590/5590 [00:00<00:00, 76745.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Conditions  Precision    Recall  F1-score  Accuracy  Hits   TP     FP  \\\n",
      "0            3   0.047101  0.048964  0.048015  0.990661   182  182   3682   \n",
      "1            5   0.034161  0.059188  0.043320  0.987426   220  220   6220   \n",
      "2            7   0.028172  0.068335  0.039896  0.984181   254  254   8762   \n",
      "3            9   0.024241  0.075599  0.036710  0.980917   281  281  11311   \n",
      "4           11   0.022092  0.084208  0.035001  0.977667   313  313  13855   \n",
      "5           13   0.020784  0.093624  0.034016  0.974424   348  348  16396   \n",
      "6           15   0.019513  0.101426  0.032730  0.971166   377  377  18943   \n",
      "7           17   0.018908  0.111380  0.032327  0.967928   414  414  21482   \n",
      "8           19   0.018388  0.121065  0.031927  0.964688   450  450  24022   \n",
      "9           21   0.017340  0.126177  0.030489  0.961404   469  469  26579   \n",
      "10          23   0.016574  0.132096  0.029453  0.958128   491  491  29133   \n",
      "11          25   0.016149  0.139898  0.028956  0.954869   520  520  31680   \n",
      "12          27   0.015672  0.146624  0.028317  0.951601   545  545  34231   \n",
      "13          29   0.015314  0.153888  0.027856  0.948337   572  572  36780   \n",
      "14          31   0.014952  0.160613  0.027357  0.945069   597  597  39331   \n",
      "15          33   0.014634  0.167339  0.026914  0.941800   622  622  41882   \n",
      "16          35   0.014330  0.173796  0.026477  0.938529   646  646  44434   \n",
      "\n",
      "      FN      TN  \n",
      "0   3535  765401  \n",
      "1   3497  762863  \n",
      "2   3463  760321  \n",
      "3   3436  757772  \n",
      "4   3404  755228  \n",
      "5   3369  752687  \n",
      "6   3340  750140  \n",
      "7   3303  747601  \n",
      "8   3267  745061  \n",
      "9   3248  742504  \n",
      "10  3226  739950  \n",
      "11  3197  737403  \n",
      "12  3172  734852  \n",
      "13  3145  732303  \n",
      "14  3120  729752  \n",
      "15  3095  727201  \n",
      "16  3071  724649  \n"
     ]
    }
   ],
   "source": [
    "evaluator = IRMetricsEvaluator(\n",
    "    recommender=recommender,\n",
    "    file_path=_TEST_SET_FILES_LIST[0],\n",
    ")\n",
    "evaluator.top_n_eval(\n",
    "    top_n_conditions=top_n_conditions,\n",
    ")\n",
    "df: DataFrame = evaluator.evlautions_summary_df()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IO] /home/taegyu/git_repo/ipirec/data/colley/test_2_purchase_list.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LOAD] test_2_purchase_list.csv: 100%|██████████| 3087/3087 [00:00<00:00, 77840.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Conditions  Precision    Recall  F1-score  Accuracy  Hits  TP     FP  \\\n",
      "0            3   0.000909  0.001932  0.001236  0.992656     3   3   3297   \n",
      "1            5   0.000727  0.002576  0.001134  0.989326     4   4   5496   \n",
      "2            7   0.001039  0.005151  0.001729  0.986005     8   8   7692   \n",
      "3            9   0.001515  0.009659  0.002619  0.982692    15  15   9885   \n",
      "4           11   0.001818  0.014166  0.003223  0.979380    22  22  12078   \n",
      "5           13   0.001678  0.015454  0.003028  0.976053    24  24  14276   \n",
      "6           15   0.001636  0.017386  0.002991  0.972729    27  27  16473   \n",
      "7           17   0.001604  0.019317  0.002963  0.969405    30  30  18670   \n",
      "8           19   0.001435  0.019317  0.002672  0.966071    30  30  20870   \n",
      "9           21   0.001558  0.023181  0.002921  0.962756    36  36  23064   \n",
      "10          23   0.001739  0.028332  0.003277  0.959447    44  44  25256   \n",
      "11          25   0.001891  0.033484  0.003580  0.956138    52  52  27448   \n",
      "12          27   0.001818  0.034771  0.003456  0.952811    54  54  29646   \n",
      "13          29   0.001724  0.035415  0.003288  0.949480    55  55  31845   \n",
      "14          31   0.001701  0.037347  0.003254  0.946156    58  58  34042   \n",
      "15          33   0.001625  0.037991  0.003117  0.942826    59  59  36241   \n",
      "16          35   0.001610  0.039923  0.003096  0.939502    62  62  38438   \n",
      "\n",
      "      FN      TN  \n",
      "0   1550  655150  \n",
      "1   1549  652951  \n",
      "2   1545  650755  \n",
      "3   1538  648562  \n",
      "4   1531  646369  \n",
      "5   1529  644171  \n",
      "6   1526  641974  \n",
      "7   1523  639777  \n",
      "8   1523  637577  \n",
      "9   1517  635383  \n",
      "10  1509  633191  \n",
      "11  1501  630999  \n",
      "12  1499  628801  \n",
      "13  1498  626602  \n",
      "14  1495  624405  \n",
      "15  1494  622206  \n",
      "16  1491  620009  \n"
     ]
    }
   ],
   "source": [
    "evaluator = IRMetricsEvaluator(\n",
    "    recommender=recommender,\n",
    "    file_path=_TEST_SET_FILES_LIST[1],\n",
    ")\n",
    "evaluator.top_n_eval(\n",
    "    top_n_conditions=top_n_conditions,\n",
    ")\n",
    "df: DataFrame = evaluator.evlautions_summary_df()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAND >> W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_count = estimator.tags_count\n",
    "users_count = estimator.users_count\n",
    "\n",
    "_RAND_W: np.ndarray = np.random.rand(users_count, tags_count, tags_count)\n",
    "print(_RAND_W)\n",
    "estimator.arr_user_idx_to_weights = _RAND_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.model.arr_tags_score = np.tanh(estimator.arr_tags_score)\n",
    "estimator.arr_user_idx_to_weights = np.tanh(estimator.arr_user_idx_to_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_S: np.ndarray = copy.deepcopy(estimator.arr_tags_score)\n",
    "_W: np.ndarray = copy.deepcopy(estimator.arr_user_idx_to_weights)\n",
    "\n",
    "# _S = np.tanh(_S)\n",
    "_S\n",
    "# model.arr_tags_score = _S\n",
    "# estimator.model.arr_tags_score = _S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _W = np.tanh(_W)\n",
    "_W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [IO] np.ndarray -- S, W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S, W >> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_str = DirectoryPathValidator.current_datetime_str()\n",
    "\n",
    "file_path = str.format(\n",
    "    \"{0}/resources/IPIRec/{1}_S_{2}.npy\",\n",
    "    WORKSPACE_HOME,\n",
    "    _FOLD_SET_ID,\n",
    "    dt_str,\n",
    ")\n",
    "__dir_path = os.path.dirname(file_path)\n",
    "if not os.path.exists(__dir_path):\n",
    "    DirectoryPathValidator.mkdir(__dir_path)\n",
    "\n",
    "with open(file=file_path, mode=\"wb\") as fout:\n",
    "    np.save(fout, estimator.model.arr_tags_score)\n",
    "    fout.close()\n",
    "\n",
    "file_path = str.format(\n",
    "    \"{0}/resources/IPIRec/{1}_W_{2}.npy\",\n",
    "    WORKSPACE_HOME,\n",
    "    _FOLD_SET_ID,\n",
    "    dt_str,\n",
    ")\n",
    "with open(file=file_path, mode=\"wb\") as fout:\n",
    "    np.save(fout, estimator.arr_user_idx_to_weights)\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >> S, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = str.format(\n",
    "    \"{0}/resources/IPIRec/{1}_S.npy\",\n",
    "    WORKSPACE_HOME,\n",
    "    _FOLD_SET_ID,\n",
    ")\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError()\n",
    "_S: np.ndarray = np.load(\n",
    "    file=file_path,\n",
    ")\n",
    "file_path = str.format(\n",
    "    \"{0}/resources/IPIRec/{1}_W.npy\",\n",
    "    WORKSPACE_HOME,\n",
    "    _FOLD_SET_ID,\n",
    ")\n",
    "_W: np.ndarray = np.load(\n",
    "    file=file_path,\n",
    ")\n",
    "estimator.model.arr_tags_score = _S\n",
    "estimator.arr_user_idx_to_weights = _W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ITER = 100\n",
    "# 39934.18908918701\n",
    "# [P->V] 39940.724917418236 >> 39939.05102884622\n",
    "__E = list()\n",
    "for decision_type in [DecisionType.E_VIEW,]:\n",
    "    for _ in range(_ITER):\n",
    "        _L = estimator._personalization_(decision_type)\n",
    "        print(f\"[{_ + 1} | {_ITER}]: {_L}\")\n",
    "        __E.append(_L)\n",
    "        if min(__E) < _L:\n",
    "            __E.clear()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ITER = 100\n",
    "# 10070.10165995471\n",
    "# 10062.877589213442 >> 10069.284461862013\n",
    "# 10062.394683458622\n",
    "# local optima가 빈번함; -- objective function에 momentum 가해야할 듯함 (근데 그러려면 2차함수를 구해줘야함)\n",
    "__E = list()\n",
    "for decision_type in [DecisionType.E_LIKE,]:\n",
    "    for _ in range(_ITER):\n",
    "        _L = estimator._personalization_(decision_type)\n",
    "        __E.append(_L)\n",
    "        print(f\"[{_ + 1} | {_ITER}]: {_L}\")\n",
    "        if min(__E) < _L:\n",
    "            __E.clear()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ITER = 100\n",
    "## 9959.982525242347\n",
    "# 10004.336950298619 >> 10005.550312827972\n",
    "# [P->S] 10005.334460052087\n",
    "# 10002.76615999305\n",
    "# V와 L보다 local optima가 더 빈번하므로, 목적함수를 좀 더 완화하도록 구성할 필요있음\n",
    "__E = list()\n",
    "for decision_type in [DecisionType.E_PURCHASE,]:\n",
    "    for _ in range(_ITER):\n",
    "        _L = estimator._personalization_(decision_type)\n",
    "        __E.append(_L)\n",
    "        print(f\"[{_ + 1} | {_ITER}]: {_L}\")\n",
    "        if min(__E) < _L:\n",
    "            __E.clear()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obs. Corr(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tags_score: np.ndarray = copy.deepcopy(estimator.arr_tags_score)\n",
    "plt.title(label=\"Tags scores\", fontsize=8.0)\n",
    "\n",
    "_min = np.min(_tags_score)\n",
    "_max = np.max(_tags_score[_tags_score < 1.0])\n",
    "_tag_names_list = list(estimator.tags_dict.keys())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "plt.xlabel(xlabel=\"Source tags name\", fontsize=4.0)\n",
    "plt.ylabel(ylabel=\"Target tags name\", fontsize=4.0)\n",
    "plt.xticks(fontsize=2.0)\n",
    "plt.yticks(fontsize=2.0)\n",
    "ax = sns.heatmap(\n",
    "    data=_tags_score,\n",
    "    vmin=_min,\n",
    "    vmax=_max,\n",
    "    cmap=\"Grays\",\n",
    "    xticklabels=_tag_names_list,\n",
    "    yticklabels=_tag_names_list,\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# \"\"\"\n",
    "tags_count = estimator.tags_count\n",
    "for _ in range(tags_count):\n",
    "    _tags_score[_][_] = 0.0\n",
    "ax = sns.clustermap(\n",
    "    _tags_score,\n",
    "    vmin=_min,\n",
    "    vmax=_max,\n",
    "    cmap=\"Grays\",\n",
    "    xticklabels=_tag_names_list,\n",
    "    yticklabels=_tag_names_list,\n",
    "    ## defaults\n",
    "    # cbar_kws=dict(use_gridspec=False, location=\"top\"),\n",
    "    # cbar_pos=(0.02, 0.8, 0.05, 0.18),\n",
    "    cbar_kws=dict(use_gridspec=False, location=\"top\"),\n",
    "    cbar_pos=(0.03, 0.85, 0.1, 0.01),\n",
    "    ## (pos_x, pos_y, len_x, len_y)\n",
    ")\n",
    "ax.tick_params(axis=\"x\", labelsize=2.0)\n",
    "ax.tick_params(axis=\"y\", labelsize=2.0)\n",
    "# \"\"\"\n",
    "\n",
    "_fig_file_path = str.format(\n",
    "    \"{0}/trunk/obs/set{1}_cmap.svg\",\n",
    "    WORKSPACE_HOME,\n",
    "    _FOLD_SET_ID,\n",
    ")\n",
    "__fig_dir_path = os.path.dirname(_fig_file_path)\n",
    "if not os.path.exists(__fig_dir_path):\n",
    "    DirectoryPathValidator.mkdir(__fig_dir_path)\n",
    "\n",
    "ax.figure.savefig(_fig_file_path)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HIER_HEATMAP\n",
    "\n",
    "_users_dist: np.ndarray = copy.deepcopy(estimator.arr_user_idx_to_weights)\n",
    "user_id = 424169\n",
    "plt.title(label=f\"W(u) = {user_id}\", fontsize=8.0)\n",
    "\n",
    "uidx = estimator.user_id_to_idx[user_id]\n",
    "if estimator.user_id_to_idx.get(user_id, -1) == -1:\n",
    "    raise KeyError()\n",
    "\n",
    "__OBS_W: np.ndarray = _users_dist[uidx]\n",
    "_min = np.min(__OBS_W)\n",
    "_max = np.max(__OBS_W[__OBS_W < 1.0])\n",
    "_tag_names_list = list(estimator.tags_dict.keys())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "plt.xlabel(xlabel=\"Source tags name\", fontsize=4.0)\n",
    "plt.ylabel(ylabel=\"Target tags name\", fontsize=4.0)\n",
    "plt.xticks(fontsize=2.0)\n",
    "plt.yticks(fontsize=2.0)\n",
    "ax = sns.heatmap(\n",
    "    data=__OBS_W,\n",
    "    vmin=_min,\n",
    "    vmax=_max,\n",
    "    cmap=\"Grays\",\n",
    "    xticklabels=_tag_names_list,\n",
    "    yticklabels=_tag_names_list,\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# \"\"\"\n",
    "tags_count = estimator.tags_count\n",
    "for _ in range(tags_count):\n",
    "    __OBS_W[_][_] = 0.0\n",
    "ax = sns.clustermap(\n",
    "    __OBS_W,\n",
    "    vmin=_min,\n",
    "    vmax=_max,\n",
    "    cmap=\"Grays\",\n",
    "    xticklabels=_tag_names_list,\n",
    "    yticklabels=_tag_names_list,\n",
    "    ## defaults\n",
    "    # cbar_kws=dict(use_gridspec=False, location=\"top\"),\n",
    "    # cbar_pos=(0.02, 0.8, 0.05, 0.18),\n",
    "    cbar_kws=dict(use_gridspec=False, location=\"top\"),\n",
    "    cbar_pos=(0.03, 0.85, 0.1, 0.01),\n",
    "    ## (pos_x, pos_y, len_x, len_y)\n",
    ")\n",
    "ax.tick_params(axis=\"x\", labelsize=2.0)\n",
    "ax.tick_params(axis=\"y\", labelsize=2.0)\n",
    "# \"\"\"\n",
    "\n",
    "_fig_file_path = str.format(\n",
    "    \"{0}/trunk/obs/set{1}_W_u{2}_cmap.svg\",\n",
    "    WORKSPACE_HOME,\n",
    "    _FOLD_SET_ID,\n",
    "    user_id,\n",
    ")\n",
    "__fig_dir_path = os.path.dirname(_fig_file_path)\n",
    "if not os.path.exists(__fig_dir_path):\n",
    "    DirectoryPathValidator.mkdir(__fig_dir_path)\n",
    "\n",
    "ax.figure.savefig(_fig_file_path)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HEATMAP\n",
    "_users_dist: np.ndarray = copy.deepcopy(estimator.arr_user_idx_to_weights)\n",
    "user_id = 424169\n",
    "plt.title(label=f\"W(u) = {user_id}\", fontsize=8.0)\n",
    "\n",
    "uidx = estimator.user_id_to_idx[user_id]\n",
    "if estimator.user_id_to_idx.get(user_id, -1) == -1:\n",
    "    raise KeyError()\n",
    "\n",
    "__OBS_W: np.ndarray = _users_dist[uidx]\n",
    "_min = np.min(__OBS_W)\n",
    "_max = np.max(__OBS_W[__OBS_W < 1.0])\n",
    "_tag_names_list = list(estimator.tags_dict.keys())\n",
    "\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "tags_count = estimator.tags_count\n",
    "for _ in range(tags_count):\n",
    "    __OBS_W[_][_] = 0.0\n",
    "\n",
    "plt.xlabel(xlabel=\"Source tags name\", fontsize=4.0)\n",
    "plt.ylabel(ylabel=\"Target tags name\", fontsize=4.0)\n",
    "plt.xticks(fontsize=2.0)\n",
    "plt.yticks(fontsize=2.0)\n",
    "ax = sns.heatmap(\n",
    "    data=__OBS_W,\n",
    "    vmin=_min,\n",
    "    vmax=_max,\n",
    "    cmap=\"Grays\",\n",
    "    xticklabels=_tag_names_list,\n",
    "    yticklabels=_tag_names_list,\n",
    ")\n",
    "ax.tick_params(axis=\"x\", labelsize=2.0)\n",
    "ax.tick_params(axis=\"y\", labelsize=2.0)\n",
    "# \"\"\"\n",
    "\n",
    "_fig_file_path = str.format(\n",
    "    \"{0}/trunk/obs/set{1}_W_u{2}.svg\",\n",
    "    WORKSPACE_HOME,\n",
    "    _FOLD_SET_ID,\n",
    "    user_id,\n",
    ")\n",
    "__fig_dir_path = os.path.dirname(_fig_file_path)\n",
    "if not os.path.exists(__fig_dir_path):\n",
    "    DirectoryPathValidator.mkdir(__fig_dir_path)\n",
    "\n",
    "ax.figure.savefig(_fig_file_path)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S * W(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HIER_HEATMAP\n",
    "user_id = 424169\n",
    "\n",
    "_tags_score: np.ndarray = copy.deepcopy(estimator.arr_tags_score)\n",
    "_tag_names_list = list(estimator.tags_dict.keys())\n",
    "\n",
    "tags_count = estimator.tags_count\n",
    "plt.title(label=f\"S * W(u) = {user_id}\", fontsize=8.0)\n",
    "\n",
    "uidx = estimator.user_id_to_idx[user_id]\n",
    "if estimator.user_id_to_idx.get(user_id, -1) == -1:\n",
    "    raise KeyError()\n",
    "\n",
    "_users_dist: np.ndarray = copy.deepcopy(estimator.arr_user_idx_to_weights)\n",
    "__OBS_W: np.ndarray = _users_dist[uidx]\n",
    "\n",
    "for _ in range(tags_count):\n",
    "    _tags_score[_][_] = 0.0\n",
    "    __OBS_W[_][_] = 0.0\n",
    "\n",
    "_WS = _tags_score * __OBS_W\n",
    "_min = np.min(_WS)\n",
    "_max = np.max(_WS[_WS < 1.0])\n",
    "ax = sns.clustermap(\n",
    "    _WS,\n",
    "    vmin=_min,\n",
    "    vmax=_max,\n",
    "    cmap=\"Grays\",\n",
    "    xticklabels=_tag_names_list,\n",
    "    yticklabels=_tag_names_list,\n",
    "    ## defaults\n",
    "    # cbar_kws=dict(use_gridspec=False, location=\"top\"),\n",
    "    # cbar_pos=(0.02, 0.8, 0.05, 0.18),\n",
    "    cbar_kws=dict(use_gridspec=False, location=\"top\"),\n",
    "    cbar_pos=(0.03, 0.85, 0.1, 0.01),\n",
    "    ## (pos_x, pos_y, len_x, len_y)\n",
    ")\n",
    "ax.tick_params(axis=\"x\", labelsize=2.0)\n",
    "ax.tick_params(axis=\"y\", labelsize=2.0)\n",
    "# \"\"\"\n",
    "\n",
    "_fig_file_path = str.format(\n",
    "    \"{0}/trunk/obs/set{1}_WS_u{2}_cmap.svg\",\n",
    "    WORKSPACE_HOME,\n",
    "    _FOLD_SET_ID,\n",
    "    user_id,\n",
    ")\n",
    "__fig_dir_path = os.path.dirname(_fig_file_path)\n",
    "if not os.path.exists(__fig_dir_path):\n",
    "    DirectoryPathValidator.mkdir(__fig_dir_path)\n",
    "\n",
    "ax.figure.savefig(_fig_file_path)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## HEATMAP\n",
    "user_id = 424169\n",
    "_tags_score: np.ndarray = copy.deepcopy(estimator.arr_tags_score)\n",
    "plt.title(label=f\"W * S ({user_id})\", fontsize=8.0)\n",
    "\n",
    "_min = np.min(_tags_score)\n",
    "_max = np.max(_tags_score[_tags_score < 1.0])\n",
    "_tag_names_list = list(estimator.tags_dict.keys())\n",
    "\n",
    "tags_count = estimator.tags_count\n",
    "_users_dist: np.ndarray = copy.deepcopy(estimator.arr_user_idx_to_weights)\n",
    "\n",
    "uidx = estimator.user_id_to_idx[user_id]\n",
    "if estimator.user_id_to_idx.get(user_id, -1) == -1:\n",
    "    raise KeyError()\n",
    "\n",
    "__OBS_W: np.ndarray = _users_dist[uidx]\n",
    "_min = np.min(__OBS_W)\n",
    "_max = np.max(__OBS_W[__OBS_W < 1.0])\n",
    "_tag_names_list = list(estimator.tags_dict.keys())\n",
    "\n",
    "for _ in range(tags_count):\n",
    "    _tags_score[_][_] = 0.0\n",
    "    __OBS_W[_][_] = 0.0\n",
    "\n",
    "_WS = _tags_score * __OBS_W\n",
    "\n",
    "plt.xlabel(xlabel=\"Source tags name\", fontsize=4.0)\n",
    "plt.ylabel(ylabel=\"Target tags name\", fontsize=4.0)\n",
    "plt.xticks(fontsize=2.0)\n",
    "plt.yticks(fontsize=2.0)\n",
    "ax = sns.heatmap(\n",
    "    data=_WS,\n",
    "    vmin=_min,\n",
    "    vmax=_max,\n",
    "    cmap=\"Grays\",\n",
    "    xticklabels=_tag_names_list,\n",
    "    yticklabels=_tag_names_list,\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "_fig_file_path = str.format(\n",
    "    \"{0}/trunk/obs/set{1}_WS_u{2}.svg\",\n",
    "    WORKSPACE_HOME,\n",
    "    _FOLD_SET_ID,\n",
    "    user_id,\n",
    ")\n",
    "__fig_dir_path = os.path.dirname(_fig_file_path)\n",
    "if not os.path.exists(__fig_dir_path):\n",
    "    DirectoryPathValidator.mkdir(__fig_dir_path)\n",
    "\n",
    "ax.figure.savefig(_fig_file_path)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVAL - Tags scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IO] /home/taegyu/git_repo/ipirec/data/colley/test_2_like_list.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LOAD] test_2_like_list.csv: 100%|██████████| 5590/5590 [00:00<00:00, 77670.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits RMSE: 0.9985384348943671\n",
      "ForAll RMSE: 0.9997043455887041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tags_scores_evaluator = TagsScoreRMSEEvaluator(\n",
    "    recommender=recommender,\n",
    "    file_path=_TEST_SET_FILES_LIST[0],\n",
    ")\n",
    "\n",
    "tags_scores_evaluator.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IO] /home/taegyu/git_repo/ipirec/data/colley/test_2_purchase_list.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LOAD] test_2_purchase_list.csv: 100%|██████████| 3087/3087 [00:00<00:00, 73054.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits RMSE: 0.9999572988390121\n",
      "ForAll RMSE: 0.9999947147357442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tags_scores_evaluator = TagsScoreRMSEEvaluator(\n",
    "    recommender=recommender,\n",
    "    file_path=_TEST_SET_FILES_LIST[1],\n",
    ")\n",
    "\n",
    "tags_scores_evaluator.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diff. of Tags Freq. - Rec. Items;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rec_tags_freq import CosineItemsTagsFreqAddPenalty\n",
    "\n",
    "rec_tags_freq_dist = CosineItemsTagsFreqAddPenalty()\n",
    "evaluator = IRMetricsEvaluator(\n",
    "    recommender=recommender,\n",
    "    file_path=_TEST_SET_FILES_LIST[0],\n",
    ")\n",
    "avg_cos_dist = rec_tags_freq_dist.tags_freq_distance(\n",
    "    test_set=evaluator.TEST_SET_LIST,\n",
    "    recommender=recommender\n",
    ")\n",
    "print(f\"L: {avg_cos_dist}\")\n",
    "\n",
    "evaluator = IRMetricsEvaluator(\n",
    "    recommender=recommender,\n",
    "    file_path=_TEST_SET_FILES_LIST[1],\n",
    ")\n",
    "avg_cos_dist = rec_tags_freq_dist.tags_freq_distance(\n",
    "    test_set=evaluator.TEST_SET_LIST,\n",
    "    recommender=recommender\n",
    ")\n",
    "print(f\"P: {avg_cos_dist}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clone -- estimator;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dummy_estimator: AdjustedBiasedCorrelationEstimator = copy.deepcopy(estimator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cfEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
